{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T09:13:27.309544Z",
     "start_time": "2021-03-23T09:13:23.751291Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3ec87d7a69d4b0cbea667b4bba71d5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Tab(children=(VBox(children=(Accordion(children=(FileUpload(value={}, accept='*.txt', descriptiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#download glove embeddings for summarization part\n",
    "#!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "#!unzip glove*.zip\n",
    "\n",
    "import re \n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from os import path\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "plt.style.use('dark_background')\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from ipywidgets import AppLayout, Button, Layout, FileUpload, HBox, Text, VBox, Tab, widgets, Output, Accordion\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class NLP_app:\n",
    "    def __init__(self):\n",
    "        #first tab\n",
    "        self.file_upload = FileUpload(accept='*.txt', multiple=False, description='Upload a file')\n",
    "        self.file_upload.style.button_color = 'lightblue'\n",
    "        self.preview_file = Button(description='Preview file')\n",
    "        self.preview_file.style.button_color = 'lightblue'\n",
    "        self.preview_file.on_click(self.on_preview_file)\n",
    "        self.clean_preview = Button(description='Clear preview')\n",
    "        self.clean_preview.style.button_color = 'salmon'\n",
    "        self.clean_preview.on_click(self.clean_your_screen)\n",
    "        \n",
    "        self.stats = Button(description='Base statistics')\n",
    "        self.stats.style.button_color = 'lightblue'\n",
    "        self.stats.on_click(self.on_stats)\n",
    "        self.freq = Button(description='Frequency distribution')\n",
    "        self.freq.style.button_color = 'lightblue'\n",
    "        self.freq.on_click(self.on_freq)\n",
    "        self.poss = Button(description='Parts of speech')\n",
    "        self.poss.style.button_color = 'lightblue'\n",
    "        self.poss.on_click(self.on_poss)\n",
    "        \n",
    "        #second tab\n",
    "        self.apply_cloud = Button(description='Generate a cloud')\n",
    "        self.apply_cloud.style.button_color = 'lightblue'\n",
    "        self.apply_cloud.on_click(self.on_apply_cloud)\n",
    "        \n",
    "        #third tab\n",
    "        self.apply_topic = Button(description='Generate the topic')\n",
    "        self.apply_topic.style.button_color = 'lightblue'\n",
    "        self.apply_topic.on_click(self.on_apply_topic)\n",
    "        \n",
    "        #fouth tab\n",
    "        self.apply_summary = Button(description='Make a summary')\n",
    "        self.apply_summary.style.button_color = 'lightblue'\n",
    "        self.apply_summary.on_click(self.on_apply_summary)\n",
    "\n",
    "        #fifth tab\n",
    "        self.apply_sentiment = Button(description='Check sentiment')\n",
    "        self.apply_sentiment.style.button_color = 'lightblue'\n",
    "        self.apply_sentiment.on_click(self.on_apply_sentiment)\n",
    "        \n",
    "        #clean all\n",
    "        self.clean_screen = Button(description='Clear all')\n",
    "        self.clean_screen.style.button_color = 'tomato'\n",
    "        \n",
    "        self.clean_screen.on_click(self.clean_your_screen)\n",
    "        \n",
    "        self.output = Output()\n",
    "    \n",
    "        self.tab = Tab(layout=Layout(width='60%', height = '100%'))\n",
    "        \n",
    "        self.accordion = Accordion(children=[\n",
    "                        self.file_upload, HBox([self.preview_file, self.clean_preview]), \n",
    "                        HBox([self.stats, self.freq, self.poss])])\n",
    "        self.accordion.set_title(0, 'Input file')\n",
    "        self.accordion.set_title(1, 'File preview')\n",
    "        self.accordion.set_title(2, 'Statistics')\n",
    "        \n",
    "        self.accordion_box = VBox([self.accordion, self.output])\n",
    "        self.word_cloud = VBox([self.apply_cloud, self.output]) \n",
    "        self.topic = VBox([self.apply_topic, self.output])\n",
    "        self.summary = VBox([self.apply_summary, self.output])\n",
    "        self.sentiment = VBox([self.apply_sentiment, self.output])\n",
    "        \n",
    "        self.children = [\n",
    "            self.accordion_box,\n",
    "            self.word_cloud,\n",
    "            self.topic,\n",
    "            self.summary,\n",
    "            self.sentiment\n",
    "        ]\n",
    "        \n",
    "        self.tab.children = self.children\n",
    "        self.tab.set_title(0, \"Upload\")\n",
    "        self.tab.set_title(1, \"Word cloud\")\n",
    "        self.tab.set_title(2, \"Topic modeling\")\n",
    "        self.tab.set_title(3, \"Text summary\")\n",
    "        self.tab.set_title(4, \"Text sentiment\")\n",
    "        \n",
    "        self.container = VBox([self.tab, self.clean_screen])\n",
    "\n",
    "    # Functions\n",
    "    def preview(self, file_upload):\n",
    "        \"\"\"Function to make content preview\"\"\"\n",
    "        for i in self.file_upload.value:\n",
    "            content = self.file_upload.value[i]['content'].decode(\"utf-8\") \n",
    "            return print(content)\n",
    "    \n",
    "    def text_stats(self, file_upload):\n",
    "        \"\"\"Function to show base statistics of the uploaded text\"\"\"\n",
    "        for i in self.file_upload.value:\n",
    "            content = self.file_upload.value[i]['content'].decode(\"utf-8\").lower()\n",
    "        tokenized_word = word_tokenize(content) \n",
    "\n",
    "        length = len(content) #num of chars\n",
    "        res = len(re.findall(r'\\w+', content)) #num of words\n",
    "        un = len(set(re.findall(r'\\w+', content))) #num of unique words\n",
    "\n",
    "        stops = [] #num of stop words\n",
    "        for i in tokenized_word:\n",
    "            if i in STOP_WORDS and i not in stops:\n",
    "                stops.append(i)\n",
    "\n",
    "        df = pd.DataFrame({'Chars': length, 'Words': res,\n",
    "                           'Unique words': un, 'Stop words': len(stops) }, index=[0])\n",
    "        \n",
    "        layout = go.Layout(title='Text statistics', height=300)\n",
    "        fig = go.Figure(data=[go.Table(\n",
    "                        header=dict(values=list(df.columns),\n",
    "                                    line_color='darkslategray',\n",
    "                                    fill_color='lightskyblue',\n",
    "                                    align='left'),\n",
    "                        cells=dict(values=[df.Chars, df.Words, df['Unique words'], df['Stop words']],\n",
    "                                   line_color='darkslategray',\n",
    "                                   fill_color='white',\n",
    "                                   align='left'))\n",
    "                        ], layout=layout)\n",
    "        fig.show() \n",
    "       \n",
    "    def freqs(self, file_upload):\n",
    "        \"\"\"Function to show word frequency in the uploaded text\"\"\"\n",
    "        for i in self.file_upload.value:\n",
    "            content = self.file_upload.value[i]['content'].decode(\"utf-8\").lower()\n",
    "        tokenized_word = word_tokenize(content) \n",
    "        fdist = FreqDist(tokenized_word)\n",
    "        df_fdist = pd.DataFrame(fdist.items(), columns=['Word', 'Frequency'])\n",
    "\n",
    "        fig = px.scatter(df_fdist, x=\"Word\", y=\"Frequency\",\n",
    "                    title=\"Frequency Distribution\",\n",
    "                    )\n",
    "        fig.show()\n",
    "    \n",
    "    def posspeech(self, file_upload):\n",
    "        \"\"\"Function to show part of speech tags in the uploaded text\"\"\"\n",
    "        for i in self.file_upload.value:\n",
    "            content = self.file_upload.value[i]['content'].decode(\"utf-8\").lower()\n",
    "        tokenized_word = word_tokenize(content) \n",
    "        tags = nltk.pos_tag(tokenized_word)\n",
    "        counts = Counter( tag for word,  tag in tags)\n",
    "\n",
    "        df_pos = pd.DataFrame.from_dict(counts, orient='index').reset_index()\n",
    "        df_pos = df_pos.rename(columns={'index':'POS', 0:'Count'})\n",
    "\n",
    "        fig = px.scatter(df_pos, x=\"POS\", y=\"Count\",\n",
    "                   title=\"Parts of speech\")\n",
    "        fig.show()\n",
    "    \n",
    "    def cloud(self, file_upload):\n",
    "        \"\"\"Function to generate a cloud\"\"\"\n",
    "        for i in self.file_upload.value:\n",
    "            content = self.file_upload.value[i]['content'].decode(\"utf-8\") \n",
    "        wordcloud = WordCloud().generate(content)\n",
    "        plt.figure(figsize=(14,8))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis(\"off\"); \n",
    "        plt.show()\n",
    "    \n",
    "    def topic_model(self, file_upload):\n",
    "        \"\"\"Function to create a topic\"\"\"\n",
    "        for i in self.file_upload.value:\n",
    "            content = self.file_upload.value[i]['content'].decode(\"utf-8\")\n",
    "\n",
    "        stop = set(stopwords.words('english'))\n",
    "        exclude = set(string.punctuation)\n",
    "        lemma = WordNetLemmatizer()\n",
    "\n",
    "        def clean(doc):\n",
    "            stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "            punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "            normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "            return normalized\n",
    "        \n",
    "        doc_complete = [content]\n",
    "        doc_clean = [clean(doc).split() for doc in doc_complete]\n",
    "        \n",
    "        # Creating the term dictionary of our corpus, where every unique term is assigned an index.\n",
    "        dictionary = corpora.Dictionary(doc_clean)\n",
    "        # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "        doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "        # Creating the object for LDA model using gensim library\n",
    "        lda = gensim.models.LdaModel\n",
    "        ldamodel = lda(doc_term_matrix, num_topics=1, id2word = dictionary, passes=50)\n",
    "        x = ldamodel.print_topics(num_topics=5, num_words=1)\n",
    "        y = (x[0][1])\n",
    "        \n",
    "        only_alpha = \"\"\n",
    "        for char in y:\n",
    "        # checking whether the char is an alphabet\n",
    "            if char.isalpha():\n",
    "                only_alpha += char    \n",
    "        return print('The main topic of the text is: ' + only_alpha)\n",
    "    \n",
    "    def generate_summary(self, file_upload):\n",
    "        \"\"\"Function to generate short summary from the text\"\"\"\n",
    "        for i in self.file_upload.value:\n",
    "            content = self.file_upload.value[i]['content'].decode(\"utf-8\") \n",
    "        sentences = tokenized_sent = sent_tokenize(content)\n",
    "        \n",
    "        #remove punctuations, numbers and special characters\n",
    "        clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
    "        # make alphabets lowercase\n",
    "        clean_sentences = [s.lower() for s in clean_sentences] \n",
    "        \n",
    "        #Extract word vectors\n",
    "        word_embeddings = {}\n",
    "        f = open('glove.6B.100d.txt', encoding='utf-8') #using glove embeddings for training\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            word_embeddings[word] = coefs\n",
    "        f.close()\n",
    "        \n",
    "        sentence_vectors = []\n",
    "        for i in clean_sentences:\n",
    "            if len(i) != 0:\n",
    "                v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "            else:\n",
    "                v = np.zeros((100,))\n",
    "            sentence_vectors.append(v) \n",
    "        #similarity matrix\n",
    "        sim_mat = np.zeros([len(sentences), len(sentences)])\n",
    "        for i in range(len(sentences)):\n",
    "            for j in range(len(sentences)):\n",
    "                if i != j:\n",
    "                    sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n",
    "    \n",
    "        #Page rank\n",
    "        nx_graph = nx.from_numpy_array(sim_mat)\n",
    "        scores = nx.pagerank(nx_graph)\n",
    "        \n",
    "        #Now, we extract the top N sentences based on their rankings for summary generation.\n",
    "        ranked_sentences = sorted(((scores[i],s) for i, s in enumerate(sentences)), reverse=True)\n",
    "        for i in range(3):\n",
    "            print(ranked_sentences[i][1])\n",
    "        \n",
    "    def generate_sentiment(self, file_upload):\n",
    "        \"\"\"Function to generate sentiment of the uploaded text\"\"\"\n",
    "        df = pd.read_csv('train/amazon_cells_labelled.txt', names=['review', 'sentiment'], sep='\\t') \n",
    "        reviews = df['review'].values\n",
    "        labels = df['sentiment'].values\n",
    "        reviews_train, reviews_test, y_train, y_test = train_test_split(reviews, labels, test_size=0.2, random_state=1000)\n",
    "\n",
    "        punctuations = string.punctuation\n",
    "        parser = English()\n",
    "        stopwords = list(STOP_WORDS)\n",
    "        def spacy_tokenizer(utterance):\n",
    "            tokens = parser(utterance)\n",
    "            return [token.lemma_.lower().strip() for token in tokens if token.text.lower().strip() not in stopwords and token.text not in punctuations]\n",
    "\n",
    "        vectorizer = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))\n",
    "        vectorizer.fit(reviews_train)\n",
    "\n",
    "        X_train = vectorizer.transform(reviews_train)\n",
    "        X_test = vectorizer.transform(reviews_test)\n",
    "\n",
    "        classifier = LogisticRegression()\n",
    "        classifier.fit(X_train, y_train)\n",
    "\n",
    "        for i in self.file_upload.value:\n",
    "            content = self.file_upload.value[i]['content'].decode(\"utf-8\")\n",
    "        new_reviews = []\n",
    "        new_reviews.append(content)\n",
    "\n",
    "        X_new = vectorizer.transform(new_reviews)\n",
    "        \n",
    "        prediction = classifier.predict(X_new)\n",
    "        if prediction == [0]:\n",
    "            plt.figure(figsize=(12,6))\n",
    "            plt.title('The overall sentiment of the text is Negative')\n",
    "            img = mpimg.imread('images/grumpy.jpeg')\n",
    "            imgplot = plt.imshow(img)\n",
    "            plt.axis(\"off\");\n",
    "            plt.show()\n",
    "    \n",
    "        elif prediction == [1]:\n",
    "            plt.figure(figsize=(12,6))\n",
    "            plt.title('The overall sentiment of the text is Positive')\n",
    "            img = mpimg.imread('images/happy.jpg')\n",
    "            imgplot = plt.imshow(img)\n",
    "            plt.axis(\"off\");\n",
    "            plt.show()\n",
    "       \n",
    "    # Combining buttons with Functions\n",
    "    def clean_your_screen(self, btn):\n",
    "        with self.output:\n",
    "            clear_output()\n",
    "        \n",
    "    def on_preview_file(self, btn):\n",
    "        with self.output:\n",
    "            clear_output()\n",
    "            self.preview(self.file_upload)  \n",
    "    \n",
    "    def on_stats(self, btn):\n",
    "        with self.output:\n",
    "            clear_output()\n",
    "            self.text_stats(self.file_upload)\n",
    "    \n",
    "    def on_freq(self, btn):\n",
    "        with self.output:\n",
    "            clear_output()\n",
    "            self.freqs(self.file_upload)\n",
    "    \n",
    "    def on_poss(self, btn):\n",
    "        with self.output:\n",
    "            clear_output()\n",
    "            self.posspeech(self.file_upload)\n",
    "    \n",
    "    def on_apply_cloud(self, btn):\n",
    "        with self.output:\n",
    "            clear_output()\n",
    "            self.cloud(self.file_upload)\n",
    "    \n",
    "    def on_apply_topic(self, btn):\n",
    "        with self.output:\n",
    "            clear_output()\n",
    "            self.topic_model(self.file_upload)\n",
    "    \n",
    "    def on_apply_summary(self, btn):\n",
    "        with self.output:\n",
    "            clear_output()\n",
    "            self.generate_summary(self.file_upload)\n",
    "     \n",
    "    def on_apply_sentiment(self, btn):\n",
    "        with self.output:\n",
    "            clear_output()\n",
    "            self.generate_sentiment(self.file_upload)\n",
    "    \n",
    "    def get_layout(self):\n",
    "        return self.container\n",
    "\n",
    "NLP_app().get_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
